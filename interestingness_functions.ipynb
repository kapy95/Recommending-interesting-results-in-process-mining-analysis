{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096406b2afc64eeead0ff221205a0ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='parsing log, completed traces :: ', max=10500, style=Progress…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# log = pm4py.read_xes('DomesticDeclarations.xes_')\n",
    "# dataframe = pm4py.convert_to_dataframe(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counter(dataframe['org:role'].dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateInterestingnessFilters(log_ct,filters,labelsDF,binsDF,columns):\n",
    "    interestFilters={}\n",
    "    for parameters,result in filters.items():\n",
    "        #print(parameters)\n",
    "        kl_scores=compute_interestingness_kl_divergence2(result,log_ct,columns,False,binsDF,labelsDF)\n",
    "        maxScore=max(kl_scores,key=itemgetter(1))\n",
    "        if maxScore[1]!=0:\n",
    "            parameters = parameters + (maxScore[0],maxScore[2])\n",
    "            interestFilters[parameters]=maxScore[1]\n",
    "        \n",
    "    interestFilters=pd.Series(interestFilters).sort_values(ascending=False).reset_index()\n",
    "    interestFilters[0]=normBoxScore(interestFilters[0])\n",
    "    \n",
    "    return interestFilters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexNumericalValue(dataframe,attributes):\n",
    "    labels={}\n",
    "    bins={}\n",
    "    \n",
    "    for attribute in attributes:\n",
    "        nbinsCase=round(freedman_diaconis(dataframe[attribute],\"number\"))\n",
    "        #widthCaseAmount=round(freedman_diaconis(dataframe[\"case:Amount\"],\"width\"))\n",
    "        discretized_case=pd.cut(dataframe[attribute].values,nbinsCase).value_counts().sort_values(ascending=False)\n",
    "        bins[attribute]=nbinsCase\n",
    "        labels[attribute]=discretized_case.index.categories\n",
    "    \n",
    "    return labels,bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freedman_diaconis(data, returnas):\n",
    "    \"\"\"\n",
    "    Use Freedman Diaconis rule to compute optimal histogram bin width. \n",
    "    ``return`` can be one of \"width\" or \"bins\", indicating whether\n",
    "    the bin width or number of bins should be returned respectively. \n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        One-dimensional array.\n",
    "\n",
    "    returns: {\"width\", \"bins\"}\n",
    "        If \"width\", return the estimated width for each histogram bin. \n",
    "        If \"bins\", return the number of bins suggested by rule.\n",
    "    \"\"\"\n",
    "    data = np.asarray(data, dtype=np.float_)\n",
    "    IQR  = stats.iqr(data, rng=(25, 75), scale=\"raw\", nan_policy=\"omit\")\n",
    "    N    = data.size\n",
    "    bw   = (2 * IQR) / np.power(N, 1/3)\n",
    "\n",
    "    if returnas==\"width\":\n",
    "        result = bw\n",
    "    elif returnas==\"number\":\n",
    "        datmin, datmax = data.min(), data.max()\n",
    "        datrng = datmax - datmin\n",
    "        result = int((datrng / bw) + 1)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normBoxScore(data):\n",
    "\n",
    "    #first the boxcox function tries to normalize the data\n",
    "    boxcox=stats.boxcox(data)#boxcox returns an array (position 0) and a value related to the boxcox transformation (position 1)    \n",
    "    #after that the zscore function is also applied to reduce the scale of the data \n",
    "    results_normalized=stats.zscore(boxcox[0])\n",
    "    \n",
    "    return results_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compaction(groups,tuples):\n",
    "    #without using pandas\n",
    "    #att=groups.reset_index().columns#reset index converts the values of attributes in columns whose names are the attributes\n",
    "    #att=len(att[att!=0])#the columns that are not 0, they are the att and with len we measure the number of attributes\n",
    "    #numberGroups=groups.size#number of groups\n",
    "    numberGroups=groups.ngroups\n",
    "    numberAtt=len(groups.keys)\n",
    "    compaction=(numberGroups*numberAtt)/tuples\n",
    "    \n",
    "    return compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretizeContinousVariable(data,nbins,labels):\n",
    "    #nbins=round(freedman_diaconis(data,\"number\"))\n",
    "    discretized_data=pd.cut(data,nbins,labels=labels)\n",
    "    df_vc=discretized_data.value_counts()\n",
    "    df_vc=df_vc.replace(0,np.nan).dropna()\n",
    "    df_vc=pd.Series(df_vc)\n",
    "    return df_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interestingness_kl_divergence(df_new,df_prev,columns,group):\n",
    "        '''\n",
    "        #Returns 1-log(3)^max_{KL_div_attr for each attribute in the current dataframe}(-KL_div_attr)\n",
    "        Returns sigmoid(max_{KL_div_attr for each attribute in the current datafame}/2-3)\n",
    "        :param dfs:\n",
    "        :param state:\n",
    "        :return:\n",
    "        '''\n",
    "        kl_distances = []\n",
    "        \n",
    "        if group==True:\n",
    "            aggregate_attributes_list = get_aggregate_attributes(state)\n",
    "            kl_attrs = aggregate_attributes_list\n",
    "            \n",
    "        else:\n",
    "            KL_DIV_EPSILON = 2 / len(df_prev) * 0.1#valor que se utilizara para sustituir valores nulos\n",
    "            kl_attrs = columns#saca los atributos del dataframe\n",
    "\n",
    "        # compute KL_divergence for each attribute\n",
    "        for attr in kl_attrs:\n",
    "            # attr_value_count1 = df_D[attr].value_counts().to_dict()\n",
    "            # attr_value_count2 = df_dt[attr].value_counts().to_dict()\n",
    "            variable = 5\n",
    "\n",
    "            if isinstance(df_new[attr][0], numbers.Number):\n",
    "                #nbins_new=freedman_diaconis(df_new[attr])\n",
    "                attr_value_count1 = Counter(df_new[attr].dropna().values)\n",
    "                attr_value_count2 = Counter(df_prev[attr].dropna().values)\n",
    "            else:\n",
    "                attr_value_count1 = Counter(df_new[attr].dropna().values)\n",
    "                attr_value_count2 = Counter(df_prev[attr].dropna().values)\n",
    "\n",
    "#             attr_value_count1 = CounterWithoutNanKeys(df_D[attr].values)\n",
    "#             attr_value_count2 = CounterWithoutNanKeys(df_dt[attr].values)\n",
    "\n",
    "            if group==True:\n",
    "                KL_DIV_EPSILON = 2 / sum(attr_value_count1.elements()) * 0.1\n",
    "\n",
    "            '''if not is_grouping:\n",
    "                num_of_NaNs_1 = len(df_D) - sum(attr_value_count1.values())\n",
    "                num_of_NaNs_2 = len(df_dt) - sum(attr_value_count2.values())'''\n",
    "\n",
    "            pk1 = []\n",
    "            pk2 = []\n",
    "            for key in attr_value_count1:\n",
    "                pk1.append(attr_value_count1[key])\n",
    "                if key in attr_value_count2:\n",
    "                    pk2.append(attr_value_count2[key])\n",
    "                else:\n",
    "                    pk2.append(KL_DIV_EPSILON)\n",
    "\n",
    "            # add the rest of attributes not in attr_value_count1\n",
    "            for key in attr_value_count2:\n",
    "                if key not in attr_value_count1:\n",
    "                    pk2.append(attr_value_count2[key])\n",
    "                    pk1.append(KL_DIV_EPSILON)\n",
    "\n",
    "            # add NaNs number for non-grouping case\n",
    "            '''if not is_grouping:\n",
    "                if num_of_NaNs_1 != 0 or num_of_NaNs_2 != 0:\n",
    "                    num_of_NaNs_1 = num_of_NaNs_1 if num_of_NaNs_1 != 0 else KL_DIV_EPSILON\n",
    "                    num_of_NaNs_2 = num_of_NaNs_2 if num_of_NaNs_2 != 0 else KL_DIV_EPSILON\n",
    "                    pk1.append(num_of_NaNs_1)\n",
    "                    pk2.append(num_of_NaNs_2)'''\n",
    "\n",
    "            attr_kl_div = entropy(pk1, pk2)\n",
    "            kl_distances.append((attr,attr_kl_div,\"kullback\"))\n",
    "        # return 1-math.log(3)**(-max(kl_distances))\n",
    "        #kl_distances.sort(key=itemgetter(1))\n",
    "        return kl_distances\n",
    "        #return max(kl_distances,key=itemgetter(1)) #1 / (1 + math.exp(-(max(kl_distances) / 2 - 3)))-> versión sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interestingness_kl_divergence2(df_new,df_prev,columns,group,nbinsVar,allLabels):\n",
    "        '''\n",
    "        #Returns 1-log(3)^max_{KL_div_attr for each attribute in the current dataframe}(-KL_div_attr)\n",
    "        Returns sigmoid(max_{KL_div_attr for each attribute in the current datafame}/2-3)\n",
    "        :param dfs:\n",
    "        :param state:\n",
    "        :return:\n",
    "        '''\n",
    "        kl_distances = []\n",
    "        \n",
    "        if group==True:\n",
    "            aggregate_attributes_list = get_aggregate_attributes(state)\n",
    "            kl_attrs = aggregate_attributes_list\n",
    "            \n",
    "        else:\n",
    "            KL_DIV_EPSILON = 2 / len(df_prev) * 0.1#valor que se utilizara para sustituir valores nulos\n",
    "            kl_attrs = columns#saca los atributos del dataframe\n",
    "\n",
    "        # compute KL_divergence for each attribute\n",
    "        for attr in kl_attrs:\n",
    "            # attr_value_count1 = df_D[attr].value_counts().to_dict()\n",
    "            # attr_value_count2 = df_dt[attr].value_counts().to_dict()\n",
    "            variable = 5\n",
    "\n",
    "            if isinstance(df_new[attr].iloc[0], numbers.Number):\n",
    "                labels=allLabels[attr]\n",
    "                nbins=nbinsVar[attr]\n",
    "                attr_value_count1 = discretizeContinousVariable(df_new[attr],nbins,labels)\n",
    "                attr_value_count1.index = attr_value_count1.index.map(str)\n",
    "                attr_value_count1=Counter(attr_value_count1)\n",
    "                \n",
    "                attr_value_count2 = discretizeContinousVariable(df_prev[attr],nbins,labels)\n",
    "                attr_value_count2.index = attr_value_count2.index.map(str)\n",
    "                attr_value_count2=Counter(attr_value_count2)\n",
    "            else:\n",
    "                attr_value_count1 = Counter(df_new[attr].dropna().values)\n",
    "                attr_value_count2 = Counter(df_prev[attr].dropna().values)\n",
    "\n",
    "#             attr_value_count1 = CounterWithoutNanKeys(df_D[attr].values)\n",
    "#             attr_value_count2 = CounterWithoutNanKeys(df_dt[attr].values)\n",
    "\n",
    "            if group==True:\n",
    "                KL_DIV_EPSILON = 2 / sum(attr_value_count1.elements()) * 0.1\n",
    "\n",
    "            '''if not is_grouping:\n",
    "                num_of_NaNs_1 = len(df_D) - sum(attr_value_count1.values())\n",
    "                num_of_NaNs_2 = len(df_dt) - sum(attr_value_count2.values())'''\n",
    "\n",
    "            pk1 = []\n",
    "            pk2 = []\n",
    "            for key in attr_value_count1:\n",
    "                pk1.append(attr_value_count1[key])\n",
    "                if key in attr_value_count2:\n",
    "                    pk2.append(attr_value_count2[key])\n",
    "                else:\n",
    "                    pk2.append(KL_DIV_EPSILON)\n",
    "\n",
    "            # add the rest of attributes not in attr_value_count1\n",
    "            for key in attr_value_count2:\n",
    "                if key not in attr_value_count1:\n",
    "                    pk2.append(attr_value_count2[key])\n",
    "                    pk1.append(KL_DIV_EPSILON)\n",
    "\n",
    "            # add NaNs number for non-grouping case\n",
    "            '''if not is_grouping:\n",
    "                if num_of_NaNs_1 != 0 or num_of_NaNs_2 != 0:\n",
    "                    num_of_NaNs_1 = num_of_NaNs_1 if num_of_NaNs_1 != 0 else KL_DIV_EPSILON\n",
    "                    num_of_NaNs_2 = num_of_NaNs_2 if num_of_NaNs_2 != 0 else KL_DIV_EPSILON\n",
    "                    pk1.append(num_of_NaNs_1)\n",
    "                    pk2.append(num_of_NaNs_2)'''\n",
    "\n",
    "            attr_kl_div = entropy(pk1, pk2)\n",
    "            kl_distances.append((attr,attr_kl_div,\"kullback\"))\n",
    "        # return 1-math.log(3)**(-max(kl_distances))\n",
    "        #kl_distances.sort(key=itemgetter(1))\n",
    "\n",
    "        return kl_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_not_system=dataframe[dataframe[\"org:resource\"]!=\"SYSTEM\"]\n",
    "\n",
    "# compute_interestingness_kl_divergence(df_not_system,dataframe,[\"org:resource\",\"case:Amount\"],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def compute_interestingness_variance(df,columns, indexes):\n",
    "    \n",
    "#     listVars=[]\n",
    "    \n",
    "#     for column in columns:\n",
    "        \n",
    "#         listValues=df[column]\n",
    "#         print(column)\n",
    "#         if isinstance(listValues[0],str)==True:\n",
    "#             frec=df.groupby(column).apply(lambda x: len(x))\n",
    "#             listVars.append((column,np.var(frec),\"variance\"))\n",
    "#         else:\n",
    "#             listVars.append((column,np.var(listValues),\"variance\"))\n",
    "        \n",
    "#     for ind in indexes:\n",
    "#         listValues=df.index.get_level_values(ind).unique()\n",
    "        \n",
    "#         if isinstance(listValues[0],str)==True:\n",
    "#             frec=df.groupby(ind).apply(lambda x: len(x))\n",
    "#             listVars.append((ind,np.var(frec),\"variance\"))\n",
    "#         else:\n",
    "#             listVars.append((ind,np.var(listValues),\"variance\"))\n",
    "        \n",
    "        \n",
    "#     #return max(listaVars,key=itemgetter(1)) returns only one\n",
    "#     return listVars.sort(key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_interestingness_variance(df,columns, indexes):\n",
    "    \n",
    "#     listVars=[]\n",
    "#     listColumnsIndex=[]\n",
    "    \n",
    "#     for column in columns:\n",
    "        \n",
    "#         listValues=df[column]\n",
    "#         print(column)\n",
    "#         if isinstance(listValues[0],str)==True:\n",
    "#             frec=df.groupby(column).apply(lambda x: len(x))\n",
    "#             listVars.append(np.var(frec))\n",
    "#             listColumnsIndex.append((column,\"variance\"))\n",
    "#         else:\n",
    "#             listVars.append(np.var(listValues))\n",
    "#             listColumnsIndex.append((column,\"variance\"))\n",
    "            \n",
    "        \n",
    "#     for ind in indexes:\n",
    "#         listValues=df.index.get_level_values(ind).unique()\n",
    "        \n",
    "#         if isinstance(listValues[0],str)==True:\n",
    "#             frec=df.groupby(ind).apply(lambda x: len(x))\n",
    "#             listVars.append(np.var(frec))\n",
    "#             listColumnsIndex.append((ind,\"variance\"))\n",
    "\n",
    "#         else:\n",
    "#             listVars.append(np.var(listValues))\n",
    "#             listColumnsIndex.append((ind,\"variance\"))\n",
    "        \n",
    "#     listVars=normBoxScore(listVars)\n",
    "#     df = pd.DataFrame({'interestValues':listVars,'columnMeasure':listColumnsIndex})\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(frec):\n",
    "    valVariance=np.var(frec)\n",
    "#     if isinstance(df[field][0],str)==True:\n",
    "#         frec=df[field].value_counts()\n",
    "#         valVariance=np.var(frec)\n",
    "\n",
    "#     else:\n",
    "#         valVariance=np.var(df[field])\n",
    "    #print(valVariance)\n",
    "    return valVariance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def shapleyFormula(fun,field,dataframe):\n",
    "    \n",
    "    args=dataframe[field].unique()\n",
    "    N=np.math.factorial(len(args))\n",
    "    shapleyValues={}\n",
    "    valuesFunc={} #first all values are calculated depending on the possible combinations\n",
    "    \n",
    "    #for example if we have {A,B,C}\n",
    "    for j in range(1,len(args)+1):#counter to generate all possible combinations\n",
    "        #1->{A}, 2->{A,B}, 3->{A,B,C}\n",
    "        #if you want to check what itertools does, execute this code:\n",
    "#         for i in itertools.combinations(['A','B','C','D'], 2):\n",
    "#             print(i)\n",
    "        for i in itertools.combinations(args, j):#generate all possible combinations of size i\n",
    "            #print(i)\n",
    "            #if i is 2-> {A,B}, {B,C}, {C,A}\n",
    "            dataframeComb=dataframe[dataframe[field].isin(i)].reset_index()#filter the rows that are included in the combination\n",
    "            joined_string = \"\".join(i)#concatenation to generate the key\n",
    "            valuesFunc[joined_string]=fun(dataframeComb,field)#calculate the value for that rows and addition of the key \n",
    "    \n",
    "    for val in args:#for each value (for example A)\n",
    "        #print(val)\n",
    "        print()\n",
    "        keys=valuesFunc.keys()#A,B,C,D,AB,CD...ABC,\n",
    "        valKeys=[key for key in keys if val in key]#get the combinations where the value is involved\n",
    "        #for instance, in the case of A: A, AB,CA, ABC\n",
    "        values=[]\n",
    "\n",
    "        for key in valKeys:#for each key (combination)\n",
    "            comb=key.replace(val,\"\")#get the case where A is not involved in S-> AB would be B, or ABC->BC\n",
    "            \n",
    "#             print(key)\n",
    "#             print(comb)\n",
    "            if comb!=\"\":#if the other case is not null (A->\"\")\n",
    "                value2=valuesFunc[comb] #val(BC)\n",
    "            else: \n",
    "                value2=0\n",
    "            #get the other combination\n",
    "            #valuesShapley1[key]->val(Pr u Xi), valuesShapley1[comb]-> Val(Pr)\n",
    "            #valuesFunc[key]-> val(ABC)\n",
    "            values.append(valuesFunc[key]-value2)#val(Pr u Xi) - Val(Pr)\n",
    "            \n",
    "        shapleyValues[val]=sum(values)/N\n",
    "        \n",
    "    return shapleyValues\n",
    "                \n",
    "#args array de valores unicos    \n",
    "#     #args=log[field].unique()#{a,b,c}\n",
    "#     valShapleys=[]\n",
    "#     for val in args:#{a}\n",
    "#         print(val)\n",
    "#         args2=args\n",
    "#         args2.remove(val)\n",
    "\n",
    "#         for z in range(0,len(args2)):\n",
    "#             #print(\"z=\"+str(z))\n",
    "            \n",
    "#             #valShapleys.append(fun(arg3))#int(A)\n",
    "            \n",
    "#             for j in range(z+1,len(args2)+1):\n",
    "#                 #print(\"j=\"+str(j))\n",
    "#                 arg3=[]#{}\n",
    "#                 arg3.append(val)#{a}\n",
    "#                 arg4=arg3+args2[z:j]\n",
    "#                 print(arg4)\n",
    "#                 #log[log[field]].value_counts/len(filtrado)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapleyFormula(variance,\"org:resource\",dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe[\"org:role\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "# dic=shapleyFormula(variance,\"org:role\",dataframe)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# dic2=shapleyFormula(variance,\"concept:name\",dataframe)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dispersion\n",
    "def schutzFormula(counts):\n",
    "    \n",
    "    #it receives value_counts\n",
    "    if sum(counts)!=0:\n",
    "        probs=counts/sum(counts)\n",
    "        numberTuples=len(probs)\n",
    "        q=1/numberTuples\n",
    "\n",
    "    #     if np.isfinite(probs)==False:\n",
    "    #         print(\"infinite\")\n",
    "        #print(counts)\n",
    "        #schutz=float(1-sum([prob-q for prob in probs])/(2*numberTuples*q))\n",
    "        schutz=1-sum([abs(prob-q) for prob in probs])/(2*numberTuples*q)\n",
    "    else:    \n",
    "        schutz=0\n",
    "        \n",
    "    return schutz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interestingness_formula(df,fun,columns,indexes,name,inverted=False):\n",
    "    #inverted is a boolen stating that the function assess low values better than high values (i.e, the order is inverted)\n",
    "    listVars=[]\n",
    "    listColumnsIndex=[]\n",
    "    listVariables=[]\n",
    "    listFun=[]\n",
    "    \n",
    "    for column in columns:\n",
    "        \n",
    "        listValues=df[column]\n",
    "        #print(column)\n",
    "        if isinstance(listValues[0],str)==True:\n",
    "            frec=df.groupby(column).apply(lambda x: len(x))\n",
    "            listVars.append(fun(frec))\n",
    "            listFun.append(name)\n",
    "            listColumnsIndex.append(column)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            listVars.append(fun(listValues))\n",
    "            listFun.append(name)\n",
    "            listColumnsIndex.append(column)\n",
    "\n",
    "            \n",
    "        \n",
    "    for ind in indexes:\n",
    "        listValues=df.index.get_level_values(ind).unique()\n",
    "        \n",
    "        if isinstance(listValues[0],str)==True:\n",
    "            frec=df.groupby(ind).apply(lambda x: len(x))\n",
    "            listVars.append(fun(frec))\n",
    "            listFun.append(name)\n",
    "            listColumnsIndex.append(ind)\n",
    "\n",
    "\n",
    "        else:\n",
    "            listVars.append(fun(listValues))\n",
    "            listFun.append(name)\n",
    "            listColumnsIndex.append(ind)\n",
    "\n",
    "    listVars=normBoxScore(listVars)\n",
    "    \n",
    "    if inverted==True:\n",
    "        listVars=listVars*-1\n",
    "        \n",
    "    df = pd.DataFrame({'interestValue':listVars,'column':listColumnsIndex,'function':listFun})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interestingness_formula2(df,fun,columns,indexes,name,allLabels,nbinsVar,inverted=False,freq=False):\n",
    "    \n",
    "    #inverted is a boolen stating that the function assess low values better than high values (i.e, the order is inverted)\n",
    "    listVars=[]\n",
    "    listColumnsIndex=[]\n",
    "    listVariables=[]\n",
    "    listFun=[]\n",
    "    \n",
    "    for column in columns:\n",
    "        \n",
    "        listValues=df[column]\n",
    "        #print(column)\n",
    "        if isinstance(listValues[0],str)==True:\n",
    "            frec=df.groupby(column).apply(lambda x: len(x))\n",
    "            listVars.append(fun(frec))\n",
    "            listFun.append(name)\n",
    "            listColumnsIndex.append(column)\n",
    "            \n",
    "        elif freq==True:\n",
    "            labels=allLabels[column]\n",
    "            nbins=nbinsVar[column]\n",
    "            numeric_freq = discretizeContinousVariableDef(df[column],nbins,labels)\n",
    "            #print(numeric_freq)\n",
    "            listVars.append(fun(numeric_freq.values))\n",
    "            listFun.append(name)\n",
    "            listColumnsIndex.append(column)\n",
    "            \n",
    "        else:\n",
    "            listVars.append(fun(listValues))\n",
    "            listFun.append(name)\n",
    "            listColumnsIndex.append(column)\n",
    "\n",
    "            \n",
    "        \n",
    "    for ind in indexes:\n",
    "        listValues=df.index.get_level_values(ind).unique()\n",
    "        \n",
    "        if isinstance(listValues[0],str)==True:\n",
    "            frec=df.groupby(ind).apply(lambda x: len(x))\n",
    "            listVars.append(fun(frec))\n",
    "            listFun.append(name)\n",
    "            listColumnsIndex.append(ind)\n",
    "\n",
    "\n",
    "        else:\n",
    "            listVars.append(fun(listValues))\n",
    "            listFun.append(name)\n",
    "            listColumnsIndex.append(ind)\n",
    "\n",
    "    listVars=normBoxScore(listVars)\n",
    "    \n",
    "    if inverted==True:\n",
    "        listVars=listVars*-1\n",
    "        \n",
    "    df = pd.DataFrame({'interestValue':listVars,'column':listColumnsIndex,'function':listFun})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interestingness_KPIs(kpis,fun,name,freq=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        kpis: it is a dictionary whose keys are the dimensions of the KPIs and the values are the results of the KPIs\n",
    "        For instance: Average cycle time of resources in the traces: [0,102,290293,1]\n",
    "        \n",
    "        fun: it is the interestingness function that will be applied\n",
    "        \n",
    "        name: name of the function to be applied\n",
    "        \n",
    "        inverted: boolean to indicate that the interestingness function order is inverse\n",
    "        \n",
    "        freq: boolean to indicate that the function requires specifically frequencies (e.g. schutz coefficient)\n",
    "    \n",
    "    Output:\n",
    "        df: dataframe with the scores of the interestingness function, which is associated to the dimensions and the measure\n",
    "    \"\"\"\n",
    "    \n",
    "    listVars=[]\n",
    "    listColumnsIndex=[]\n",
    "    listDimension=list(kpis.keys())\n",
    "    listFun=[name for i in range(len(listDimension))]\n",
    "    binned=[]\n",
    "    \n",
    "    for dimension,kpi in kpis.items():\n",
    "        if type(kpi)!=tuple:\n",
    "            interestResult=fun(kpi)\n",
    "            listVars.append(interestResult)\n",
    "            binned.append(False)\n",
    "            \n",
    "        elif type(kpi)==tuple:\n",
    "            if freq==False:\n",
    "                interestResult=fun(kpi[0])                \n",
    "                listVars.append(interestResult)\n",
    "                binned.append(False)\n",
    "            else:\n",
    "                interestResult=fun(kpi[1])\n",
    "                listVars.append(interestResult)\n",
    "                binned.append(True)\n",
    "\n",
    "    listVars=normBoxScore(listVars)\n",
    "        \n",
    "    df = pd.DataFrame({'interestValue':listVars,'Dimension':listDimension,'function':listFun,'binned':binned})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_res_act=dataframe.groupby([\"org:resource\"])[\"concept:name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schutzFormula(counts_res_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bergerFormula(kpis):\n",
    "#     for column in columns:\n",
    "\n",
    "#         max_val=df[column].value_counts().sort_values(ascending=False)[[0]]\n",
    "#         counts.append(max_val[[0]].values[0])\n",
    "#         results.append(str(max_val.index[0]))\n",
    "#         listColumns.append(column)\n",
    "#         listFun.append(\"support\")\n",
    "       \n",
    "#     counts=normBoxScore(counts)\n",
    "#     df = pd.DataFrame({'interestValue':counts,'column':listColumns,'function':listFun,\"concreteValue\":results})\n",
    "    \n",
    "    listVars=[]\n",
    "    listConcreteValues=[]\n",
    "    listDimension=list(kpis.keys())\n",
    "    listFun=[\"bergerFormula\" for i in range(len(listDimension))]\n",
    "    binned=[]\n",
    "    \n",
    "    for dimension,kpi in kpis.items():\n",
    "        if type(kpi)!=tuple:\n",
    "            value_max=kpi.sort_values(ascending=False)[[0]]\n",
    "            interestResult=value_max[0]/sum(kpi)#support\n",
    "            concreteValue=value_max.index[0]\n",
    "            listConcreteValues.append(concreteValue)\n",
    "            listVars.append(interestResult)\n",
    "            binned.append(False)\n",
    "            \n",
    "        elif type(kpi)==tuple:\n",
    "            value_max=kpi[1].sort_values(ascending=False)[[0]]\n",
    "            interestResult=value_max[0]/sum(kpi[1])#support\n",
    "            concreteValue=value_max.index[0]\n",
    "            listConcreteValues.append(concreteValue)\n",
    "            listVars.append(interestResult)\n",
    "            binned.append(True)\n",
    "\n",
    "    listVars=normBoxScore(listVars)    \n",
    "    df = pd.DataFrame({'interestValue':listVars,'Dimension':listDimension,'function':listFun,\"concreteValue\":listConcreteValues,'binned':binned})\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
